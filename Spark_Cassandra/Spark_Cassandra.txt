L’objectif de ce TP est de manipuler des données stockées dans une base Cassandra en utilisant Docker et Apache Spark pour effectuer des traitements distribués.


1) Installation de Docker via le lien suivant: 
   https://www.docker.com/products/docker-desktop. Il contient tous les composants nécessaires à l’utilisation de Docker.

2) Lancer docker-desktop


3) Installer un conteneur Cassandra avec Docker : 

   docker run --name mon-cassandra -p 3000:9042  -d cassandra:latest

4) Associer Spark et Cassandra : 
   
   pyspark --packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.0

5) Etablir une connexion avec Cassandra
  
   session = SparkSession.builder \
        .appName("NFE204") \
        .config("spark.cassandra.connection.host", "localhost") \
        .config("spark.cassandra.connection.port", "3000") \
        .config("spark.sql.extensions", "com.datastax.spark.connector.CassandraSparkExtensions") \
        .getOrCreate()
#---------------------------------------------------------------------------------
                 ##  Création de la base de données pour Cassandra  ##

Les données à utiliser sont disponible ici: http://b3d.bdpedia.fr/files/restaurants.zip

a) Création de la base de données nommée resto_NY 

   CREATE KEYSPACE IF NOT EXISTS resto_NY
      WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor': 1};

c) Sous csqlsh, sélectionner la base de données resto_NY,  afin de pouvoir faires des requêtes sur cette base de données

   USE resto_NY;

d) Création des tables (Column Family pour Cassandra) Restaurant et Inspection à partir du schéma suivant :

-----------------------------------------------------------------------------
 CREATE TABLE Restaurant (                                                     
   id INT, Name VARCHAR, borough VARCHAR, BuildingNum VARCHAR, Street VARCHAR,
   ZipCode INT, Phone text, CuisineType VARCHAR,
   PRIMARY KEY ( id )
 ) ;

 CREATE INDEX fk_Restaurant_cuisine ON Restaurant ( CuisineType ) ;

 CREATE TABLE Inspection (
   idRestaurant INT, InspectionDate date, ViolationCode VARCHAR,
   ViolationDescription VARCHAR, CriticalFlag VARCHAR, Score INT, GRADE VARCHAR,
   PRIMARY KEY ( idRestaurant, InspectionDate )
 ) ;

CREATE INDEX fk_Inspection_Restaurant ON Inspection ( Grade ) ;
----------------------------------------------------------------------------------

c)  Vérifier si les tables ont bien été créées (sous cqlsh):
    
    DESC Restaurant;
    DESC Inspection;

d) Import les données dans ces tables: 

   - Décompresser le fichier "restaurants.zip"

     unzip restaurants.zip

   - Dans la console (machine locale, pas docker), copier les fichiers sous « Docker » (container "Cassandra"):
     
     docker cp chemin_du_fichier/restaurants.csv    docker-container-ID:/
     docker cp chemin_du_fichier/restaurants_inspections.csv    docker-container-ID:/ 

e) Dans la console cqlsh, importer les fichiers "restaurants.csv" et "restaurants_inspections.csv"
  
   use resto_NY ;

   COPY Restaurant (id, name, borough, buildingnum, street, zipcode, phone, cuisinetype)
        FROM '/restaurants.csv' WITH DELIMITER=',';

   COPY Inspection (idrestaurant, inspectiondate, violationcode, violationdescription, criticalflag, score, grade)
        FROM '/restaurants_inspections.csv' WITH DELIMITER=',';


f) Vérifier le contenu des tables:

   SELECT count(*) FROM Restaurant;
   SELECT count(*) FROM Inspection;
#---------------------------------------------------------------------------------

                        ## Revenons dans Spark ##

6) Afficher un extrait des restaurants(après avoir charger les données):
  
   restaurants_df = session.read.format("org.apache.spark.sql.cassandra") \
                .options(table="restaurant", keyspace="resto_ny") \
                .load()
   restaurants_df.show()


7) Forcer la conservation du dataframe: 
   
   restaurants_df.cache()


8) inspections_df = session.read.format("org.apache.spark.sql.cassandra") \
                .options(table="inspection", keyspace="resto_ny") \
                .load()



                                         Traitements Spark/Cassandra


1) La commande suivante ne conserve que trois colonnes: 

   restaus_simples = restaurants_df.select("name", "phone", "cuisinetype")
   restaus_simples.show()


2) Effectue une sélection (avec le mot-clé filter, correspondant au where de SQL):

   manhattan = restaurants_df.filter("borough =  'MANHATTAN'")
   manhattan.show()

3) Effectuer une opération de jointure:
   
   restaus_inspections = restaurants_df.join(inspections_df, restaurants_df.id == inspections_df.idrestaurant)
   restaus_inspections.cache()

4) Agrégation pour regrouper et compter des restaurants par arrondissement (borough):

   comptage_par_borough = restaus_inspections.groupBy("borough").count()  

5  Moyenne des notes des restaurants de tapas:

   from pyspark.sql import functions as sf

   comptage_tapas = restaurants_df.filter("cuisinetype > 'Tapas'") \
        .join(inspections_df, restaurants_df.id == inspections_df.idrestaurant) \
        .groupBy("name") \
        .agg(sf.avg("score"))