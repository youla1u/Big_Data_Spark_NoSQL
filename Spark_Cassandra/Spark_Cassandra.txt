# =========================================================
# Spark + Cassandra avec Docker : ingestion & traitements
# =========================================================

# 0) Objectif
# ----------- 
# Manipuler des données dans Cassandra via Docker et les traiter avec Spark
# (Spark Cassandra Connector). Jeu de données : restaurants NYC.

# 1) Installation & démarrage Docker Desktop
# ------------------------------------------
# - Installer Docker Desktop (Windows/macOS) depuis : https://www.docker.com/products/docker-desktop
# - Lancer Docker Desktop et vérifier qu’il tourne (docker info).

# 2) Lancer un conteneur Cassandra
# --------------------------------
# - Démarre Cassandra exposé en local sur le port 3000 (mappé au 9042 interne).
docker run --name mon-cassandra -p 3000:9042 -d cassandra:latest

# - (Astuce) Suivre les logs de démarrage jusqu’au "Startup complete":
#   docker logs -f mon-cassandra

# 3) Associer Spark et Cassandra (shell PySpark)
# ----------------------------------------------
# - Lancer PySpark avec le connecteur DataStax (version compatible Spark 3.5.x).
pyspark --packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.0

# 4) Session Spark configurée pour Cassandra
# ------------------------------------------
# - Connexion au host local et au port mappé (3000).
from pyspark.sql import SparkSession
session = SparkSession.builder \
    .appName("NFE204") \
    .config("spark.cassandra.connection.host", "localhost") \
    .config("spark.cassandra.connection.port", "3000") \
    .config("spark.sql.extensions", "com.datastax.spark.connector.CassandraSparkExtensions") \
    .getOrCreate()

# -------------------------------------------------------------------------------
#                ## Création & alimentation de la base Cassandra ##
# -------------------------------------------------------------------------------

# 5) Récupération des données
# ---------------------------
# - Télécharger l’archive des restaurants NYC puis la décompresser.
#   (depuis la machine hôte)
#   wget http://b3d.bdpedia.fr/files/restaurants.zip
#   unzip restaurants.zip

# 6) Accès au shell CQL (cqlsh)
# -----------------------------
# - Ouvrir cqlsh à l’intérieur du conteneur (utilise le port 9042 interne) :
#   docker exec -it mon-cassandra cqlsh
#   (ou depuis l’hôte : cqlsh 127.0.0.1 3000 si cqlsh est installé localement)

# 7) Création du keyspace & sélection
# -----------------------------------
# - Keyspace mono-noeud (SimpleStrategy, RF=1) puis USE.
CREATE KEYSPACE IF NOT EXISTS resto_NY
  WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor': 1 };
USE resto_NY;

# 8) Création des tables + index
# ------------------------------
# - Deux tables : Restaurant (clé primaire = id) et Inspection
#   (clé primaire composite = (idRestaurant, InspectionDate)).
CREATE TABLE Restaurant (
  id INT, Name VARCHAR, borough VARCHAR, BuildingNum VARCHAR, Street VARCHAR,
  ZipCode INT, Phone text, CuisineType VARCHAR,
  PRIMARY KEY (id)
);
CREATE INDEX fk_Restaurant_cuisine ON Restaurant (CuisineType);

CREATE TABLE Inspection (
  idRestaurant INT, InspectionDate date, ViolationCode VARCHAR,
  ViolationDescription VARCHAR, CriticalFlag VARCHAR, Score INT, GRADE VARCHAR,
  PRIMARY KEY (idRestaurant, InspectionDate)
);
CREATE INDEX fk_Inspection_Restaurant ON Inspection (Grade);

# 9) Vérifications de schéma (dans cqlsh)
# ---------------------------------------
DESC Restaurant;
DESC Inspection;

# 10) Copier les fichiers CSV dans le conteneur
# ---------------------------------------------
# - Récupère l’ID du conteneur si besoin : docker ps
# - Copie depuis l’hôte vers la racine du conteneur :
docker cp chemin_du_fichier/restaurants.csv mon-cassandra:/
docker cp chemin_du_fichier/restaurants_inspections.csv mon-cassandra:/

# 11) Import CSV -> Cassandra (dans cqlsh)
# ----------------------------------------
USE resto_NY;

COPY Restaurant (id, name, borough, buildingnum, street, zipcode, phone, cuisinetype)
  FROM '/restaurants.csv' WITH DELIMITER=',';

COPY Inspection (idrestaurant, inspectiondate, violationcode, violationdescription, criticalflag, score, grade)
  FROM '/restaurants_inspections.csv' WITH DELIMITER=',';

# 12) Comptages rapides (contrôle de chargement)
# ----------------------------------------------
SELECT count(*) FROM Restaurant;
SELECT count(*) FROM Inspection;

# -------------------------------------------------------------------------------
#                               ## Côté Spark ##
# -------------------------------------------------------------------------------

# 13) Charger les tables Cassandra en DataFrames Spark
# ----------------------------------------------------
restaurants_df = session.read.format("org.apache.spark.sql.cassandra") \
    .options(table="restaurant", keyspace="resto_ny") \
    .load()
restaurants_df.show()        # aperçu
restaurants_df.cache()       # 14) mise en cache pour réutilisations
restaurants_df.count()       # matérialisation du cache (optionnel)

inspections_df = session.read.format("org.apache.spark.sql.cassandra") \
    .options(table="inspection", keyspace="resto_ny") \
    .load()

# -------------------------------------------------------------------------------
#                         ## Traitements Spark/Cassandra ##
# -------------------------------------------------------------------------------

# 15) Projection de colonnes (équivalent SELECT col1,col2,...)
# ------------------------------------------------------------
restaus_simples = restaurants_df.select("name", "phone", "cuisinetype")
restaus_simples.show()

# 16) Filtre (équivalent WHERE)
# -----------------------------
manhattan = restaurants_df.filter("borough = 'MANHATTAN'")
manhattan.show()

# 17) Jointure restaurants ↔ inspections
# --------------------------------------
from pyspark.sql import functions as F
restaus_inspections = restaurants_df.join(
    inspections_df,
    restaurants_df.id == inspections_df.idrestaurant,
    how="inner"
)
restaus_inspections.cache()

# 18) Agrégation : nombre de restaurants par borough
# --------------------------------------------------
comptage_par_borough = restaus_inspections.groupBy("borough").count()
comptage_par_borough.show()

# 19) Moyenne des scores pour les restaurants de TAPAS
# ----------------------------------------------------
# NOTE: le filtre dans l’énoncé utilise "cuisinetype > 'Tapas'", ce qui fait
# une comparaison lexicographique. Pour sélectionner la cuisine "Tapas",
# on préfère l’égalité (insensible à la casse si besoin).
from pyspark.sql.functions import col, lower, avg

comptage_tapas = restaurants_df \
    .filter(lower(col("cuisinetype")) == "tapas") \
    .join(inspections_df, restaurants_df.id == inspections_df.idrestaurant, "inner") \
    .groupBy("name") \
    .agg(avg("score").alias("avg_score")) \
    .orderBy(col("avg_score").asc_nulls_last())

comptage_tapas.show(20, truncate=False)

# (Variante stricte, sans lower):
# comptage_tapas = restaurants_df.filter("cuisinetype = 'Tapas'") \
#   .join(inspections_df, restaurants_df.id == inspections_df.idrestaurant) \
#   .groupBy("name").agg(F.avg("score").alias("avg_score"))

# -------------------------------------------------------------------------------
# Remarques & bonnes pratiques
# ----------------------------
# - Cassandra : les imports COPY sont pratiques pour un unique nœud. Pour des
#   volumes importants/cluster, préférer des outils dédiés (Spark, sstableloader).
# - Index secondaires : utiles pour des requêtes ciblées mais à manier avec parcimonie.
# - Spark cache() : toujours "réchauffer" le cache avec count()/show() si vous
#   voulez bénéficier du caching dans les étapes suivantes.
# - Jointures : en production, penser aux clés de partition et aux stratégies
#   d’optimisation (broadcast join si petit DF, predicates pushdown, etc.).
# =========================================================
